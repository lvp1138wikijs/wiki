<!--
title: Management Servers 4
description: 
published: true
date: 2021-04-27T00:01:41.103Z
tags: 
editor: ckeditor
dateCreated: 2021-04-26T23:37:50.467Z
-->

<h2>Ceph Admin Server</h2>
<p>Now we have 3 monitors ready to install ceph and create the cluster. In order to do this, we need an admin server.&nbsp;</p>
<p>OS installed: CentOS 7. It is installed through our installation portal. Login to the server and set the proper hostname. Then do the following</p>
<h4>Install the Packages</h4>
<p><code>yum install epel-release -y</code><br><code>yum update -y</code><br><code>yum install lldpd python-setuptools -y</code><br><code>systemctl enable lldpd</code></p>
<p>&nbsp;</p>
<h5>Add the Ceph Repository</h5>
<pre><code class="language-plaintext">cat &gt;&gt; /etc/yum.repos.d/ceph.repo &lt;&lt;END
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-luminous/el7/x86_64/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
 
[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-luminous/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
 
[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-luminous/el7/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
END</code></pre>
<h5>Configure Networks</h5>
<p>The Admin needs a Public Network, Private &amp; Ceph storage IPs.&nbsp;</p>
<blockquote>
  <p>Las Cluster, the private network binded on vlan 8. In the other zones it binded on vlan 2.&nbsp;</p>
  <p>Las Cluster, <span style="font-family:Arial, Helvetica, sans-serif;">All other zones the public internet interface is set on VLAN&nbsp;4 ( bond 0.4 ) but in Las zone, it depends upon the subnet assigned to the server.</span></p>
  <p>Ceph network binded on vlan 9 in all zones</p>
</blockquote>
<p><strong>Sample interfaces files</strong></p>
<pre><code class="language-plaintext"># cat ifcfg-bond0.4
DEVICE=bond0.4
VLAN=yes
ONBOOT=yes
VLAN_ID=4
BOOTPROTO=static
IPADDR=72.18.198.9
NETMASK=255.255.255.0
GATEWAY=72.18.198.1

# cat ifcfg-bond0.8
DEVICE=bond0.8
VLAN=yes
ONBOOT=yes
VLAN_ID=8
BOOTPROTO=static
IPADDR=10.1.1.103
NETMASK=255.255.255.0
GATEWAY=10.1.1.1

# cat ifcfg-bond0.9
DEVICE=bond0.9
VLAN=yes
ONBOOT=yes
VLAN_ID=9
BOOTPROTO=static
IPADDR=10.255.0.198
NETMASK=255.255.240.0
</code></pre>
<blockquote>
  <p>The IPaddress and Gateway will be different on each zone.</p>
</blockquote>
<h5>Create Ceph User</h5>
<p><code>useradd -d /home/cephuser -m cephuser</code><br><code>passwd cephuser</code><br><code>echo "cephuser &nbsp;ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser</code><br><code>chmod 0440 /etc/sudoers.d/cephuser</code></p>
<h5>LLDP configuration</h5>
<p>We have installed the package already. Just configure it. <mark class="marker-yellow">Replace the system_name.</mark></p>
<p><code>echo 'configure system description _system_name_' &gt; /etc/lldpd.d/lldpcli.conf</code><br><code>echo 'configure system hostname _system_name_' &gt;&gt; /etc/lldpd.d/lldpcli.conf</code><br><code>systemctl start lldpd ; systemctl enable lldpd ; systemctl status lldpd</code><br>&nbsp;</p>
<h5><strong>Configure hostnames in /etc/hosts &amp;&nbsp; /etc/ssh/ssh_config</strong></h5>
<p>Ceph nodes must be able to resolve short hostnames, not just fully qualified domain names. We can edit it in /etc/hosts. Modify the /etc/ssh/ssh_config and /etc/hosts file of your ceph-deploy admin node so that ceph-deploy can log in to Ceph nodes as the user you created without requiring you to specify --username each time you execute ceph-deploy.</p>
<pre><code class="language-plaintext"># Sample /etc/hosts -  Add like this - ceph internal IP hostname
====================================================================

#admin 
10.255.0.49 ceph-dal-admin-a1 

#Monitors 
10.255.0.30 ceph-dal-mon-a1 
10.255.0.31 ceph-dal-mon-a2 
10.255.0.32 ceph-dal-mon-a3 

#OSD 
10.255.0.22 ceph-dal-osd-a1 
10.255.0.23 ceph-dal-osd-a2 
10.255.0.26 ceph-dal-osd-a5 


# Sample /etc/ssh/ssh_config  - add the entries like this.&nbsp;
==========================================================

Host ceph-dal-mon-a1 
   Hostname ceph-dal-mon-a1 
   User cephuser 

Host ceph-dal-mon-a2 
   Hostname ceph-dal-mon-a2 
   User cephuser 

Host ceph-dal-mon-a3 
   Hostname ceph-dal-mon-a3 
   User cephuser 

Host ceph-dal-a1-osd 
   Hostname ceph-dal-a1-osd 
   User cephuser 

Host ceph-dal-a2-osd 
   Hostname ceph-dal-a2-osd 
   User cephuser 
</code></pre>
<h5>Copy the SSH Keys</h5>
<p>Since ceph-deploy will not prompt for a password, you must generate SSH keys on the admin node and distribute the public key to each Ceph node.</p>
<p>Run this from the admin node</p>
<p><code>su cephuser</code><br><code>ssh-keygen</code><br><code>ssh-copy-id cephuser@hostname ( ceph-dal-mon-a1, ceph-dal-mon-a2, etc )</code></p>
<p>All set and now the admin, mons &amp; osds hosts are ready to make ceph cluster</p>
<h2>Ceph Cluster Installation</h2>
<p>First, we have to install the “ceph-deploy” utility in the <strong>Admin node</strong>. We are using this utility to install, create and control the cluster in all other VMs</p>
<p>Run this in the ceph Admin node as the root user.</p>
<p><code>yum update -y ; yum install ceph-deploy -y</code></p>
<p>Administration of the cluster is done entirely from the admin node. Switch to 'cephuser' and move to a dedicated directory to collect the files that ceph-deploy will generate. This will be the working directory for any further use of ceph-depoy</p>
<p>Run these commands from ceph admin node</p>
<p><code>su cephuser</code><br><code>mkdir /home/cephuser/ceph-cluster ; cd /home/cephuser/ceph-cluster</code></p>
<p>Next, we have to <strong>Deploy the monitor node</strong>(s)</p>
<blockquote>
  <p><strong>Syntax: </strong>ceph-deploy new monitor1 monitor2 monitor3</p>
</blockquote>
<p><code>ceph-deploy new ceph-dal-mon-a1 ceph-dal-mon-a2 ceph-dal-mon-a3</code></p>
<p>Then install <strong>ceph on all nodes </strong>( ceph admin, monitors, osds )</p>
<blockquote>
  <p><strong>Syntax</strong>: ceph-deploy install --release luminous {ceph_nodes}</p>
</blockquote>
<p><code>ceph-deploy install --release luminous ceph-dal-admin-a1 ceph-dal-mon-a1 ceph-dal-mon-a2 ceph-dal-mon-a3 ceph-dal-osd-a1 ceph-dal-osd-a2</code></p>
<p>Then add the <strong>initial monitor(s) and gather the keys</strong></p>
<p><code>ceph-deploy mon create-initial</code></p>
<p>Then copy over the necessary files to all the nodes</p>
<p><code>ceph-deploy admin ceph-dal-admin-a1 ceph-dal-mon-a1 ceph-dal-mon-a2 ceph-dal-mon-a3 ceph-dal-osd-a1 ceph-dal-osd-a2</code></p>
<p>&nbsp;</p>
<h3><strong>OSD Creation</strong></h3>
<p>Use <strong>disk list</strong> to list the additional hard disks connected to the OSD nodes<br><code>ceph-deploy disk list osd-host-name&nbsp;</code></p>
<p>Use <strong>disk zap</strong> command to format and mount the disks<br><code>ceph-deploy disk zap {osd-server-name}:{disk-name}</code></p>
<p>Use <strong>osd prepare</strong> to prepare the OSDs.<br><code>ceph-deploy osd prepare {osdserver1:disk-name}</code></p>
<p>Example:</p>
<p><code>ceph-deploy disk list ceph-dal-osd-a1</code><br><code>ceph-deploy disk zap ceph-dal-osd-a1:/dev/sda</code><br><code>ceph-deploy osd prepare ceph-dal-osd-a1:sda</code>&nbsp;</p>
<p>You can also use multiple disks at a time like this</p>
<p><code>ceph-deploy disk zap ceph-dal-osd-a1:/dev/sda ceph-dal-osd-a1:/dev/sdb</code><br><code>ceph-deploy osd prepare ceph-dal-osd-a1:sda ceph-dal-osd-a1:sdb</code><br><br>That’s it. You can use these commands to see the health of Cluster</p>
<p><code>ceph status</code><br><code>ceph health</code></p>
