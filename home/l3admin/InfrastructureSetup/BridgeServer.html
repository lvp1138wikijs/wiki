<!--
title: Bridge Server
description: 
published: true
date: 2021-06-01T15:53:23.530Z
tags: 
editor: ckeditor
dateCreated: 2021-06-01T15:53:23.530Z
-->

<h1>Setup Bridge Servers with Floating IPs</h1>
<p>The bridge servers are the servers that provide the internet connection to all hypervisors, ceph monitor &amp; osd servers. It acts as gateway servers.&nbsp;</p>
<h3>Requirements</h3>
<ul>
  <li>Two Dedicated servers with IPs from the same subnet</li>
  <li>Install CentOS 7</li>
</ul>
<h3>Installation &amp; Configuration</h3>
<h5>Yum Packages</h5>
<pre><code class="language-plaintext">yum -y install net-utils net-snmp telnet net-snmp-utils whois emacs rsync \
htop hdparm munin-node ntfs-3g dstat mtr pciutils tree ncurses-devel ntfsprogs nbd \
traceroute zip tmpwatchm logrotate parted unzip gdisk e4fsprogs wget cronie \
perl-libwww-perl  epel-release iptables-services nmap bzip2 ntp nano dstat sysstat \
nmap bzip2 ntp nano dstat sysstat net-tools bind-utils smartmontools yum-utils 

yum update -y ; yum install lldpd -y

systemctl stop firewalld
systemctl disable firewalld
systemctl stop NetworkManager
systemctl disable NetworkManager
systemctl enable iptables
</code></pre>
<p>&nbsp;</p>
<h5>Set Hostname &amp; enable IP forward</h5>
<pre><code class="language-plaintext">hostname las-bridge1
echo las-bridge1 &gt; /etc/hostname
echo 1 &gt; /proc/sys/net/ipv4/ip_forward
echo net.ipv4.ip_forward=1 &gt;&gt; /etc/sysctl.conf
sysctl -p</code></pre>
<h5>Configure Network</h5>
<p>The server must need a public network and a private network.</p>
<p><code>Bridge1 - Public IP: 72.18.207.170, Private IP: 10.1.1.2</code><br><code>Bridge2 - Public IP: 72.18.207.171, Private IP: 10.1.1.3</code></p>
<p><code>Floating IP: Public IP: 72.18.207.172, Private IP: 10.1.1.1</code></p>
<p><strong>Sample interfaces files</strong></p>
<pre><code class="language-plaintext"># cat ifcfg-bond0.207
DEVICE=bond0.207
VLAN=yes
ONBOOT=yes
VLAN_ID=207
BOOTPROTO=static
IPADDR=72.18.207.170
NETMASK=255.255.255.0
GATEWAY=72.18.207.129

# cat ifcfg-bond0.8
DEVICE=bond0.8
VLAN=yes
ONBOOT=yes
VLAN_ID=8
BOOTPROTO=static
IPADDR=10.1.1.2
NETMASK=255.255.255.0</code></pre>
<h2>Install Pacemaker High Availability Cluster</h2>
<h6>Add Host Entries</h6>
<pre><code class="language-plaintext">cat &gt;&gt; /etc/hosts &lt;&lt;END
10.1.1.2 las-bridge1
10.1.1.3 las-bridge2
END</code></pre>
<h6>Install PaceMaker</h6>
<pre><code class="language-plaintext">yum -y update
yum -y install  pcs fence-agents-all

systemctl start pcsd.service
systemctl enable pcsd.service
systemctl enable corosync.service</code></pre>
<h6>Set a password for hacluster user on each host &amp; Use that password to authenticate to the nodes that will make up the cluster</h6>
<pre><code class="language-plaintext">echo serverpoint | passwd --stdin hacluster
pcs cluster auth las-bridge1 las-bridge2 -u hacluster -p serverpoint --force</code></pre>
<h6>Create and name the cluster. Then, start it and enable all components to auto-start at boot time</h6>
<pre><code class="language-plaintext">pcs cluster setup --start --name las_gateway las-bridge1 las-bridge2
pcs cluster enable --all
pcs cluster status
pcs status nodes
pcs status corosync</code></pre>
<h6>Cluster Configuration</h6>
<p>To check the configuration for errors run <code>crm_verify -L -V</code>, and there are still shows errors</p>
<pre><code class="language-plaintext">crm_verify -L -V
----------
error: unpack_resources: Resource start-up disabled since no STONITH resources have been defined
error: unpack_resources: Either configure some or disable STONITH with the stonith-enabled option
error: unpack_resources: NOTE: Clusters with shared data need STONITH to ensure data integrity
Errors found during check: config not valid
------------</code></pre>
<p>Then run &nbsp;</p>
<p><code>pcs property set stonith-enabled=false</code><br><code>pcs property set no-quorum-policy=ignore</code></p>
<h6>Set Virtual IP address</h6>
<p><code>pcs resource create VIP1 ocf:heartbeat:IPaddr2 ip=10.1.1.1 cidr_netmask=32 nic=bond0.8 op monitor interval=30s</code><br><code>pcs resource create VIP2 ocf:heartbeat:IPaddr2 ip=72.18.207.172 cidr_netmask=32 nic=bond0.207 op monitor interval=30s</code></p>
<h6>Check the Pacemaker Status</h6>
<p>Run the following commands to check the status &amp; property list</p>
<p><code>pcs status</code><br><code>pcs property list</code></p>
<pre><code class="language-plaintext"># pcs status
Cluster name: las_gateway
Stack: corosync
Current DC: las-bridge1 (version 1.1.23-1.el7_9.1-9acf116022) - partition with quorum
Last updated: Tue Jun  1 08:50:56 2021
Last change: Fri Mar 26 13:39:18 2021 by hacluster via crmd on las-bridge1

2 nodes configured
2 resource instances configured

Online: [ las-bridge1 las-bridge2 ]

Full list of resources:

 VIP1   (ocf::heartbeat:IPaddr2):       Started las-bridge1
 VIP2   (ocf::heartbeat:IPaddr2):       Started las-bridge2

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
</code></pre>
<h6>Destroy Virtual IPs &amp; Cluster</h6>
<pre><code class="language-plaintext">pcs resource delete VIP1 --force
pcs cluster disable --all
pcs cluster destroy --all
pcs cluster status</code></pre>
<p>&nbsp;</p>
