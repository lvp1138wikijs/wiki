<!--
title: Ceph Admin 
description: Setup and intallation instruction of ceph admin server
published: true
date: 2021-06-01T15:32:19.726Z
tags: ceph admin
editor: ckeditor
dateCreated: 2021-04-28T19:05:24.259Z
-->

<h2>Ceph Admin Server</h2>
<p>Now we have 3 monitors ready to install ceph and create the cluster. In order to do this, we need an admin server.&nbsp;</p>
<p>OS installed: CentOS 7. It is installed through our installation portal. Login to the server and set the proper hostname. Then do the following</p>
<h4>Install the Packages</h4>
<p><code>yum install epel-release -y</code><br><code>yum update -y</code><br><code>yum install lldpd python-setuptools -y</code><br><code>systemctl enable lldpd</code></p>
<p>&nbsp;</p>
<h5>Add the Ceph Repository</h5>
<pre><code class="language-plaintext">cat &gt;&gt; /etc/yum.repos.d/ceph.repo &lt;&lt;END
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-luminous/el7/x86_64/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
 
[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-luminous/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
 
[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-luminous/el7/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1
END</code></pre>
<h5>Configure Networks</h5>
<p>The Admin needs a Public Network, Private &amp; Ceph storage IPs.&nbsp;</p>
<blockquote>
  <p>Las Cluster, the private network binded on vlan 8. In the other zones it binded on vlan 2.&nbsp;</p>
  <p>Las Cluster, <span style="font-family:Arial, Helvetica, sans-serif;">All other zones the public internet interface is set on VLAN&nbsp;4 ( bond 0.4 ) but in Las zone, it depends upon the subnet assigned to the server.</span></p>
  <p>Ceph network binded on vlan 9 in all zones</p>
</blockquote>
<p><strong>Sample interfaces files</strong></p>
<pre><code class="language-plaintext"># cat ifcfg-bond0.4
DEVICE=bond0.4
VLAN=yes
ONBOOT=yes
VLAN_ID=4
BOOTPROTO=static
IPADDR=72.18.198.9
NETMASK=255.255.255.0
GATEWAY=72.18.198.1

# cat ifcfg-bond0.8
DEVICE=bond0.8
VLAN=yes
ONBOOT=yes
VLAN_ID=8
BOOTPROTO=static
IPADDR=10.1.1.103
NETMASK=255.255.255.0

# cat ifcfg-bond0.9
DEVICE=bond0.9
VLAN=yes
ONBOOT=yes
VLAN_ID=9
BOOTPROTO=static
IPADDR=10.255.0.198
NETMASK=255.255.240.0
</code></pre>
<blockquote>
  <p>The IPaddress and Gateway will be different on each zone.</p>
</blockquote>
<h5>Create Ceph User</h5>
<p><code>useradd -d /home/cephuser -m cephuser</code><br><code>passwd cephuser</code><br><code>echo "cephuser &nbsp;ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser</code><br><code>chmod 0440 /etc/sudoers.d/cephuser</code></p>
<h5>Install Ceph-Deploy</h5>
<p>We have to install the “ceph-deploy” utility in the <strong>Admin node</strong>. We are using this utility to install, create and control the cluster in all other servers in the cluster.</p>
<p>Run this in the Ceph Admin node as the root user.</p>
<p><code>yum update -y ; yum install ceph-deploy -y</code></p>
<p>&nbsp;</p>
<h5>LLDP configuration</h5>
<p>We have installed the package already. Just configure it. <mark class="marker-yellow">Replace the system_name.</mark></p>
<p><code>echo 'configure system description _system_name_' &gt; /etc/lldpd.d/lldpcli.conf</code><br><code>echo 'configure system hostname _system_name_' &gt;&gt; /etc/lldpd.d/lldpcli.conf</code><br><code>systemctl start lldpd ; systemctl enable lldpd ; systemctl status lldpd</code><br>&nbsp;</p>
<h5><strong>Configure hostnames in /etc/hosts &amp;&nbsp; /etc/ssh/ssh_config</strong></h5>
<p>Ceph nodes must be able to resolve short hostnames, not just fully qualified domain names. We can edit it in /etc/hosts. Modify the /etc/ssh/ssh_config and /etc/hosts file of your ceph-deploy admin node so that ceph-deploy can log in to Ceph nodes as the user you created without requiring you to specify --username each time you execute ceph-deploy.</p>
<pre><code class="language-plaintext"># Sample /etc/hosts -  Add like this - ceph internal IP hostname
====================================================================

#admin 
10.255.0.49 ceph-dal-admin-a1 

#Monitors 
10.255.0.30 ceph-dal-mon-a1 
10.255.0.31 ceph-dal-mon-a2 
10.255.0.32 ceph-dal-mon-a3 

#OSD 
10.255.0.22 ceph-dal-osd-a1 
10.255.0.23 ceph-dal-osd-a2 
10.255.0.26 ceph-dal-osd-a5 


# Sample /etc/ssh/ssh_config  - add the entries like this.&nbsp;
==========================================================

Host ceph-dal-mon-a1 
   Hostname ceph-dal-mon-a1 
   User cephuser 

Host ceph-dal-mon-a2 
   Hostname ceph-dal-mon-a2 
   User cephuser 

Host ceph-dal-mon-a3 
   Hostname ceph-dal-mon-a3 
   User cephuser 

Host ceph-dal-a1-osd 
   Hostname ceph-dal-a1-osd 
   User cephuser 

Host ceph-dal-a2-osd 
   Hostname ceph-dal-a2-osd 
   User cephuser 
</code></pre>
<h5>Copy the SSH Keys</h5>
<p>Since ceph-deploy will not prompt for a password, you must generate SSH keys on the admin node and distribute the public key to each Ceph node.</p>
<p>Run this from the admin node</p>
<p><code>su cephuser</code><br><code>ssh-keygen</code><br><code>ssh-copy-id cephuser@hostname ( ceph-dal-mon-a1, ceph-dal-mon-a2, etc )</code></p>
<p>All set and now the admin, mons &amp; osds hosts are ready to make <a href="/home/l3admin/InfrastructureSetup/Ceph/CephCluster">ceph cluster</a></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
